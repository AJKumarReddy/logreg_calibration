{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Logistic Regression & Calibration — Student Lab\n",
        "\n",
        "We focus on *probabilities*, not just accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def check(name: str, cond: bool):\n",
        "    if not cond:\n",
        "        raise AssertionError(f'Failed: {name}')\n",
        "    print(f'OK: {name}')\n",
        "\n",
        "rng = np.random.default_rng(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 0 — Synthetic imbalanced data\n",
        "We simulate logits and labels with imbalance and miscalibration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "base_rate 0.0712\n",
            "OK: in_0_1\n"
          ]
        }
      ],
      "source": [
        "def make_probs(n=5000, base_rate=0.05, logit_scale=1.0, miscalibration=1.0):\n",
        "    # Generate true probabilities via latent logits\n",
        "    z = logit_scale * rng.standard_normal(n)\n",
        "    # shift to get desired base rate approximately\n",
        "    z = z + np.log(base_rate/(1-base_rate))\n",
        "    p_true = 1/(1+np.exp(-z))\n",
        "    y = (rng.random(n) < p_true).astype(int)\n",
        "    # observed model probs are miscalibrated by scaling logits\n",
        "    z_model = miscalibration * z\n",
        "    p_model = 1/(1+np.exp(-z_model))\n",
        "    return y, p_model, p_true\n",
        "\n",
        "y, p_model, p_true = make_probs(miscalibration=2.0)\n",
        "print('base_rate', y.mean())\n",
        "check('in_0_1', np.all((p_model>=0) & (p_model<=1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1 — Metrics\n",
        "\n",
        "### Task 1.1: Confusion matrix metrics at a threshold\n",
        "Implement precision, recall, F1 at threshold t.\n",
        "\n",
        "# HINT:\n",
        "- y_hat = (p>=t)\n",
        "- TP/FP/FN\n",
        "\n",
        "**Checkpoint:** Why is accuracy misleading under imbalance?\n",
        "\n",
        "since the dataset is imbalanced, lets say 95 yes's to 5 no's. even if the model predicts all the outcomes as yes, it would still get 95% accuracy. so accuracy in an imbalanced dataset is misleading. we have to focus on no's as well.\n",
        "\n",
        "in short, in an imbalced dataset, model is biased towards the majority class.\n",
        "\n",
        "Hence we are using other metrices also to get better understanding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'tp': 2, 'tn': 4642, 'fp': 2, 'fn': 354, 'accuracy': 0.9287999999999998, 'precision': 0.499999999999875, 'recall': 0.005617977528089872, 'f1': 0.011111111111089075}\n"
          ]
        }
      ],
      "source": [
        "def metrics_at_threshold(y, p, t):\n",
        "    # TODO\n",
        "    y = np.asarray(y, dtype=int)\n",
        "    p = np.asarray(p, dtype=float)\n",
        "    y_hat = (p > t).astype(int) # predicted labels based on threshold t\n",
        "    \n",
        "    tp = int(np.sum((y_hat == 1) & (y == 1))) # true positives\n",
        "    tn = int(np.sum((y_hat == 0) & (y == 0))) # true negatives\n",
        "    fp = int(np.sum((y_hat == 1) & (y == 0))) # false positives\n",
        "    fn = int(np.sum((y_hat == 0) & (y == 1))) # false negatives\n",
        "    \n",
        "    # computing derived metrics using standard formulas\n",
        "    accuracy = (tp + tn) / (len(y) + 1e-12) # to avoid division by zero\n",
        "    precision = tp / (tp + fp + 1e-12) \n",
        "    recall = tp / (tp + fn + 1e-12)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall + 1e-12)\n",
        "    return {\n",
        "        'tp': tp,\n",
        "        'tn': tn,\n",
        "        'fp': fp,\n",
        "        'fn': fn,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }\n",
        "\n",
        "m = metrics_at_threshold(y, p_model, t=0.5)\n",
        "print(m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1.2: PR curve area (approx)\n",
        "Compute a simple PR-AUC approximation by sorting thresholds.\n",
        "\n",
        "# HINT:\n",
        "- sort by p desc\n",
        "- compute precision/recall at each cut\n",
        "\n",
        "**Interview Angle:** when is PR-AUC preferable to ROC-AUC?\n",
        "\n",
        "PR-AUC preferable to ROC-AUC when there is a huge data imbalance, true positives are our major focus area not true negetives. because PR-RUC does not use true negetives when computing. hence cases where we can ignore true negetives PR-AUC is preferable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pr_auc 0.12058854890412112\n",
            "OK: finite\n"
          ]
        }
      ],
      "source": [
        "def pr_curve(y, p):\n",
        "    # TODO: return arrays (recall, precision)\n",
        "    order = np.argsort(-p) # sorting indices by descending predicted probabilities\n",
        "    y = np.asarray(y, dtype=int) # ensure y is an array\n",
        "    y_sorted = y[order] # sorting y according to the sorted indices\n",
        "    tp = np.cumsum(y_sorted==1) # cumulative true positives\n",
        "    fp = np.cumsum(y_sorted==0) # cumulative false positives\n",
        "    \n",
        "    precision = tp / (tp + fp + 1e-12)\n",
        "    recall = tp / (tp + tp[-1] + 1e-12)\n",
        "    return recall, precision\n",
        "\n",
        "def auc_trapz(x, y):\n",
        "    # TODO\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    y = np.asarray(y, dtype=float)\n",
        "    return np.trapz(y, x) # integrating y with respect to x using trapezoidal rule\n",
        "    \n",
        "rec, prec = pr_curve(y, p_model)\n",
        "pr_auc = auc_trapz(rec, prec)\n",
        "print('pr_auc', pr_auc)\n",
        "check('finite', np.isfinite(pr_auc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2 — Calibration\n",
        "\n",
        "### Task 2.1: Reliability curve + ECE\n",
        "\n",
        "Bin probabilities and compute:\n",
        "- avg predicted prob per bin\n",
        "- empirical accuracy per bin\n",
        "- ECE = sum (bin_weight * |acc - conf|)\n",
        "\n",
        "# HINT:\n",
        "- np.digitize\n",
        "\n",
        "**FAANG gotcha:** model can have good ranking but bad calibration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ECE 0.056267450617459955\n",
            "OK: ece_finite\n"
          ]
        }
      ],
      "source": [
        "def reliability_bins(y, p, n_bins=10):\n",
        "    # TODO: return (bin_acc, bin_conf, bin_frac)\n",
        "    y = np.asarray(y, dtype=int)\n",
        "    p = np.asarray(p, dtype=float)\n",
        "    bin_edges = np.linspace(0, 1, n_bins + 1)\n",
        "    b = np.digitize(p, bin_edges[1:-1], right=False)\n",
        "    \n",
        "    bin_acc = np.zeros(n_bins)\n",
        "    bin_conf = np.zeros(n_bins)\n",
        "    bin_frac = np.zeros(n_bins)\n",
        "    \n",
        "    for i in range(n_bins):\n",
        "        mask = (b == i)\n",
        "        if mask.any():\n",
        "            bin_acc[i] = y[mask].mean()\n",
        "            bin_conf[i] = p[mask].mean()\n",
        "            bin_frac[i] = mask.mean()\n",
        "        else:\n",
        "            bin_acc[i] = 0.0\n",
        "            bin_conf[i] = 0.0\n",
        "            bin_frac[i] = 0.0\n",
        "    return bin_acc, bin_conf, bin_frac\n",
        "\n",
        "def ece(bin_acc, bin_conf, bin_frac):\n",
        "    # TODO\n",
        "    return float(np.sum(np.abs(bin_acc - bin_conf) * bin_frac))\n",
        "\n",
        "bin_acc, bin_conf, bin_frac = reliability_bins(y, p_model, n_bins=10)\n",
        "ECE = ece(bin_acc, bin_conf, bin_frac)\n",
        "print('ECE', ECE)\n",
        "check('ece_finite', np.isfinite(ECE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2.2: Temperature scaling\n",
        "\n",
        "We assume p_model came from logits z_model. Approximate logits via logit(p).\n",
        "Then find temperature T that minimizes NLL on validation split: sigmoid(z/T).\n",
        "\n",
        "# HINT:\n",
        "- logit(p)=log(p/(1-p))\n",
        "- grid search T over [0.5..5]\n",
        "\n",
        "**Checkpoint:** Why does scaling logits preserve ranking?\n",
        "\n",
        "logits preserve ranking because in a montonic transformations all the elements are scaled by a same factor. so their relative order did not change. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best_T 1.969387755102041\n",
            "ECE_before 0.05789031077500049 ECE_after 0.006327853058047911\n"
          ]
        }
      ],
      "source": [
        "def logit(p, eps=1e-12):\n",
        "    p = np.clip(p, eps, 1-eps)\n",
        "    return np.log(p/(1-p))\n",
        "\n",
        "def nll(y, p, eps=1e-12):\n",
        "    p = np.clip(p, eps, 1-eps)\n",
        "    return float(-np.mean(y*np.log(p) + (1-y)*np.log(1-p)))\n",
        "\n",
        "# TODO: split into val and fit T\n",
        "idx = rng.permutation(len(y))\n",
        "val = idx[: len(y)//2]\n",
        "test = idx[len(y)//2:]\n",
        "\n",
        "z = logit(p_model)\n",
        "\n",
        "Ts = np.linspace(0.5, 5.0, 50)\n",
        "best_nll = float('inf')\n",
        "best_T = None\n",
        "for T in Ts:\n",
        "    pT = 1/(1+np.exp(-(z[val]/T)))\n",
        "    nll_T = nll(y[val], pT)\n",
        "    if nll_T < best_nll:\n",
        "        best_nll = nll_T\n",
        "        best_T = T\n",
        "        \n",
        "\n",
        "print('best_T', best_T)\n",
        "# apply temperature on test\n",
        "p_cal = 1/(1+np.exp(-(z[test]/best_T)))\n",
        "ECE_before = ece(*reliability_bins(y[test], p_model[test], 10))\n",
        "ECE_after = ece(*reliability_bins(y[test], p_cal, 10))\n",
        "print('ECE_before', ECE_before, 'ECE_after', ECE_after)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3 — Thresholding with costs\n",
        "\n",
        "### Task 3.1: Pick threshold minimizing cost\n",
        "Cost = c_fp*FP + c_fn*FN\n",
        "\n",
        "# TODO: sweep thresholds and pick best.\n",
        "\n",
        "**Interview Angle:** map model probabilities to business decisions.\n",
        "\n",
        "Thresholding helps with cost but not accuracy. In cases where high caution is required and false positives are allowed to ceratain degree (like fraud detection) Thresholding is helpful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "t* 0.006 cost 2626.0\n"
          ]
        }
      ],
      "source": [
        "def best_threshold_cost(y, p, c_fp=1.0, c_fn=10.0):\n",
        "    # TODO\n",
        "    y = np.asarray(y, dtype=int)\n",
        "    p = np.asarray(p, dtype=float)\n",
        "    ts = np.linspace(0, 1, 501)\n",
        "    best_cost = float('inf')\n",
        "    best_t = 0.5\n",
        "    for t in ts:\n",
        "        y_hat = (p > t).astype(int)\n",
        "        fp = np.sum((y_hat == 1) & (y == 0))\n",
        "        fn = np.sum((y_hat == 0) & (y == 1))\n",
        "        cost = c_fp * fp + c_fn * fn\n",
        "        if cost < best_cost:\n",
        "            best_cost = cost\n",
        "            best_t = float(t)\n",
        "    return best_t, float(best_cost)        \n",
        "\n",
        "t_star, cost_star = best_threshold_cost(y, p_model, c_fp=1.0, c_fn=10.0)\n",
        "print('t*', t_star, 'cost', cost_star)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Submission Checklist\n",
        "- All TODOs completed\n",
        "- ECE computed\n",
        "- Temperature scaling applied\n",
        "- Threshold recommendation written\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
