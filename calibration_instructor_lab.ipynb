{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {"name": "python3", "display_name": "Python 3"},
    "language_info": {"name": "python"}
  },
  "cells": [
    {"cell_type":"markdown","metadata":{},"source":[
      "# Logistic Regression & Calibration â€” Instructor (Solutions + Rationale)" 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "import numpy as np\n",
      "\n",
      "def check(name: str, cond: bool):\n",
      "    if not cond:\n",
      "        raise AssertionError(f'Failed: {name}')\n",
      "    print(f'OK: {name}')\n",
      "\n",
      "rng = np.random.default_rng(0)"
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def make_probs(n=5000, base_rate=0.05, logit_scale=1.0, miscalibration=1.0):\n",
      "    z = logit_scale * rng.standard_normal(n)\n",
      "    z = z + np.log(base_rate/(1-base_rate))\n",
      "    p_true = 1/(1+np.exp(-z))\n",
      "    y = (rng.random(n) < p_true).astype(int)\n",
      "    z_model = miscalibration * z\n",
      "    p_model = 1/(1+np.exp(-z_model))\n",
      "    return y, p_model, p_true\n",
      "\n",
      "y, p_model, p_true = make_probs(miscalibration=2.0)\n",
      "print('base_rate', y.mean())"
    ]},

    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def metrics_at_threshold(y, p, t):\n",
      "    y = y.astype(int)\n",
      "    yhat = (p >= t).astype(int)\n",
      "    tp = int(np.sum((yhat==1) & (y==1)))\n",
      "    fp = int(np.sum((yhat==1) & (y==0)))\n",
      "    fn = int(np.sum((yhat==0) & (y==1)))\n",
      "    prec = tp / (tp + fp + 1e-12)\n",
      "    rec = tp / (tp + fn + 1e-12)\n",
      "    f1 = 2*prec*rec / (prec+rec+1e-12)\n",
      "    return {'tp':tp,'fp':fp,'fn':fn,'precision':prec,'recall':rec,'f1':f1}\n",
      "\n",
      "print(metrics_at_threshold(y, p_model, 0.5))\n",
      "# Rationale: for imbalance, precision/recall capture tradeoff better than accuracy." 
    ]},

    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def pr_curve(y, p):\n",
      "    order = np.argsort(-p)\n",
      "    y_sorted = y[order]\n",
      "    tp = np.cumsum(y_sorted == 1)\n",
      "    fp = np.cumsum(y_sorted == 0)\n",
      "    prec = tp / (tp + fp + 1e-12)\n",
      "    rec = tp / (tp[-1] + 1e-12)\n",
      "    return rec, prec\n",
      "\n",
      "def auc_trapz(x, y):\n",
      "    # assume x increasing\n",
      "    return float(np.trapz(y, x))\n",
      "\n",
      "rec, prec = pr_curve(y, p_model)\n",
      "pr_auc = auc_trapz(rec, prec)\n",
      "print('pr_auc', pr_auc)\n",
      "# Rationale: PR is more sensitive to minority class performance." 
    ]},

    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def reliability_bins(y, p, n_bins=10):\n",
      "    y = y.astype(int)\n",
      "    edges = np.linspace(0, 1, n_bins+1)\n",
      "    b = np.digitize(p, edges[1:-1], right=False)\n",
      "    bin_acc = np.zeros(n_bins)\n",
      "    bin_conf = np.zeros(n_bins)\n",
      "    bin_frac = np.zeros(n_bins)\n",
      "    for i in range(n_bins):\n",
      "        mask = (b == i)\n",
      "        if mask.any():\n",
      "            bin_acc[i] = y[mask].mean()\n",
      "            bin_conf[i] = p[mask].mean()\n",
      "            bin_frac[i] = mask.mean()\n",
      "    return bin_acc, bin_conf, bin_frac\n",
      "\n",
      "def ece(bin_acc, bin_conf, bin_frac):\n",
      "    return float(np.sum(bin_frac * np.abs(bin_acc - bin_conf)))\n",
      "\n",
      "ECE = ece(*reliability_bins(y, p_model, 10))\n",
      "print('ECE', ECE)\n",
      "# Rationale: ECE measures calibration gap; ranking can be good even if calibration is poor." 
    ]},

    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def logit(p, eps=1e-12):\n",
      "    p = np.clip(p, eps, 1-eps)\n",
      "    return np.log(p/(1-p))\n",
      "\n",
      "def nll(y, p, eps=1e-12):\n",
      "    p = np.clip(p, eps, 1-eps)\n",
      "    return float(-np.mean(y*np.log(p) + (1-y)*np.log(1-p)))\n",
      "\n",
      "idx = rng.permutation(len(y))\n",
      "val = idx[: len(y)//2]\n",
      "test = idx[len(y)//2:]\n",
      "\n",
      "z = logit(p_model)\n",
      "\n",
      "Ts = np.linspace(0.5, 5.0, 50)\n",
      "best_T = None\n",
      "best_loss = float('inf')\n",
      "for T in Ts:\n",
      "    pT = 1/(1+np.exp(-(z[val]/T)))\n",
      "    L = nll(y[val], pT)\n",
      "    if L < best_loss:\n",
      "        best_loss = L\n",
      "        best_T = T\n",
      "\n",
      "p_cal = 1/(1+np.exp(-(z[test]/best_T)))\n",
      "ECE_before = ece(*reliability_bins(y[test], p_model[test], 10))\n",
      "ECE_after = ece(*reliability_bins(y[test], p_cal, 10))\n",
      "print('best_T', best_T)\n",
      "print('ECE_before', ECE_before, 'ECE_after', ECE_after)\n",
      "# Rationale: temperature scaling rescales logits, improving calibration while preserving ranking." 
    ]},

    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def best_threshold_cost(y, p, c_fp=1.0, c_fn=10.0):\n",
      "    ts = np.linspace(0, 1, 501)\n",
      "    best_t = 0.5\n",
      "    best_cost = float('inf')\n",
      "    for t in ts:\n",
      "        yhat = (p >= t).astype(int)\n",
      "        fp = np.sum((yhat==1) & (y==0))\n",
      "        fn = np.sum((yhat==0) & (y==1))\n",
      "        cost = c_fp*fp + c_fn*fn\n",
      "        if cost < best_cost:\n",
      "            best_cost = cost\n",
      "            best_t = float(t)\n",
      "    return best_t, float(best_cost)\n",
      "\n",
      "t_star, cost_star = best_threshold_cost(y, p_model, c_fp=1.0, c_fn=10.0)\n",
      "print('t*', t_star, 'cost', cost_star)\n",
      "# Rationale: thresholding is a decision problem; optimize costs, not accuracy." 
    ]}
  ]
}
